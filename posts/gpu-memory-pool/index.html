<!DOCTYPE html>
<html lang="en-us">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta itemprop="name" content="GPU Memory Pools in D3D12">
<meta itemprop="description" content="Basics of GPU Memory Integrated/UMA GPUs Dedicated/NUMA GPUs How It Works In D3D12 Common Patterns in D3D12 Textures And The Two-Step Upload Working With The COPY Queue Allocating Staging Memory What About Buffers? What About DirectStorage? Results From My Testing App CPU Write Performance CPU Read Performance GPU Read Performance, Normal Access GPU Read Performance, Non-Coalesced Access GPU Read Performance, Various Buffer Sizes Conclusion When the monkey&rsquo;s paw granted our wish for lower-level/explicit graphics APIs, one of the consequences was that we were much more directly exposed to the fact that GPUs can have their own separate set of physical memory."><meta itemprop="datePublished" content="2022-07-25T00:00:00+00:00" />
<meta itemprop="dateModified" content="2022-07-25T00:00:00+00:00" />
<meta itemprop="wordCount" content="8195"><meta itemprop="image" content="https://therealmjp.github.io/images/gpu-memory-pools/buffer_upload_copy_queue.png">
<meta itemprop="keywords" content="Graphics,DX12," /><meta property="og:title" content="GPU Memory Pools in D3D12" />
<meta property="og:description" content="Basics of GPU Memory Integrated/UMA GPUs Dedicated/NUMA GPUs How It Works In D3D12 Common Patterns in D3D12 Textures And The Two-Step Upload Working With The COPY Queue Allocating Staging Memory What About Buffers? What About DirectStorage? Results From My Testing App CPU Write Performance CPU Read Performance GPU Read Performance, Normal Access GPU Read Performance, Non-Coalesced Access GPU Read Performance, Various Buffer Sizes Conclusion When the monkey&rsquo;s paw granted our wish for lower-level/explicit graphics APIs, one of the consequences was that we were much more directly exposed to the fact that GPUs can have their own separate set of physical memory." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://therealmjp.github.io/posts/gpu-memory-pool/" /><meta property="og:image" content="https://therealmjp.github.io/images/gpu-memory-pools/buffer_upload_copy_queue.png" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-07-25T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://therealmjp.github.io/images/gpu-memory-pools/buffer_upload_copy_queue.png"/>

<meta name="twitter:title" content="GPU Memory Pools in D3D12"/>
<meta name="twitter:description" content="Basics of GPU Memory Integrated/UMA GPUs Dedicated/NUMA GPUs How It Works In D3D12 Common Patterns in D3D12 Textures And The Two-Step Upload Working With The COPY Queue Allocating Staging Memory What About Buffers? What About DirectStorage? Results From My Testing App CPU Write Performance CPU Read Performance GPU Read Performance, Normal Access GPU Read Performance, Non-Coalesced Access GPU Read Performance, Various Buffer Sizes Conclusion When the monkey&rsquo;s paw granted our wish for lower-level/explicit graphics APIs, one of the consequences was that we were much more directly exposed to the fact that GPUs can have their own separate set of physical memory."/>

	<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">
	<link rel="mask-icon" href="/safari-pinned-tab.svg" color="">
	<link rel="shortcut icon" href="/favicon.ico">

	<title>GPU Memory Pools in D3D12</title>
	<link rel="stylesheet" href="https://therealmjp.github.io/css/style.min.9ae64a8094d0d100100f0725503238784f798a59c57003700be4d8b0645124e9.css" integrity="sha256-muZKgJTQ0QAQDwclUDI4eE95ilnFcANwC+TYsGRRJOk=">
</head>

<body id="page">
	
	<header id="site-header" class="animated slideInUp faster">
		<div class="hdr-wrapper section-inner">
			<div class="hdr-left">
				<div class="site-branding">
					<a href="https://therealmjp.github.io/">The Danger Zone</a>
				</div>
				<nav class="site-nav hide-in-mobile">
					
				<a href="https://therealmjp.github.io/posts/">Posts</a>
				<a href="https://therealmjp.github.io/about/">About</a>
				<a href="https://therealmjp.github.io/publications/">Publications</a>

				</nav>
			</div>
			<div class="hdr-right hdr-icons">
				<button id="img-btn" class="hdr-btn" title="Featured Image"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-image"><rect x="3" y="3" width="18" height="18" rx="2" ry="2"></rect><circle cx="8.5" cy="8.5" r="1.5"></circle><polyline points="21 15 16 10 5 21"></polyline></svg></button><span class="hdr-social hide-in-mobile"><a href="https://github.com/TheRealMJP" target="_blank" rel="noopener me" title="Github"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a><a href="https://twitter.com/mynameismjp" target="_blank" rel="noopener me" title="Twitter"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"></path></svg></a><a href="mailto:mpettineo@gmail.com" target="_blank" rel="noopener me" title="Email"><svg xmlns="http://www.w3.org/2000/svg" class="feather" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"></path><polyline points="22,6 12,13 2,6"></polyline></svg></a></span><button id="menu-btn" class="hdr-btn" title="Menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line></svg></button>
			</div>
		</div>
	</header>
	<div id="mobile-menu" class="animated fast">
		<ul>
			<li><a href="https://therealmjp.github.io/posts/">Posts</a></li>
			<li><a href="https://therealmjp.github.io/about/">About</a></li>
			<li><a href="https://therealmjp.github.io/publications/">Publications</a></li>
		</ul>
	</div>


    <div>
    <script src="/js/chart.min.js"></script>
    <script src="/js/chart_common.js"></script>
</div>



	<div class="bg-img"></div>
	<main class="site-main section-inner animated fadeIn faster">
		<article class="thin">
			<header class="post-header">
				<div class="post-meta"><span>Jul 25, 2022</span></div>
				<h1>GPU Memory Pools in D3D12</h1>
			</header>
			<div class="content">
				<div>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#basics-of-gpu-memory">Basics of GPU Memory</a>
      <ul>
        <li><a href="#integrateduma-gpus">Integrated/UMA GPUs</a></li>
        <li><a href="#dedicatednuma-gpus">Dedicated/NUMA GPUs</a></li>
      </ul>
    </li>
    <li><a href="#how-it-works-in-d3d12">How It Works In D3D12</a></li>
    <li><a href="#common-patterns-in-d3d12">Common Patterns in D3D12</a>
      <ul>
        <li><a href="#textures-and-the-two-step-upload">Textures And The Two-Step Upload</a></li>
        <li><a href="#working-with-the-copy-queue">Working With The COPY Queue</a></li>
        <li><a href="#allocating-staging-memory">Allocating Staging Memory</a></li>
        <li><a href="#what-about-buffers">What About Buffers?</a></li>
        <li><a href="#what-about-directstorage">What About DirectStorage?</a></li>
      </ul>
    </li>
    <li><a href="#results-from-my-testing-app">Results From My Testing App</a>
      <ul>
        <li><a href="#cpu-write-performance">CPU Write Performance</a></li>
        <li><a href="#cpu-read-performance">CPU Read Performance</a></li>
        <li><a href="#gpu-read-performance-normal-access">GPU Read Performance, Normal Access</a></li>
        <li><a href="#gpu-read-performance-non-coalesced-access">GPU Read Performance, Non-Coalesced Access</a></li>
        <li><a href="#gpu-read-performance-various-buffer-sizes">GPU Read Performance, Various Buffer Sizes</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<p>When the <a href="https://en.wikipedia.org/wiki/The_Monkey%27s_Paw">monkey&rsquo;s paw</a> granted our wish for lower-level/explicit graphics APIs, one of the consequences was that we were much more directly exposed to the fact that GPUs can have their own separate set of physical memory. Anybody that&rsquo;s ever had to purchase a dedicated video card for their PC can probably tell you that such a video card has its own set of physical RAM chips on the board that are reserved for the GPU. Meanwhile the integrated GPUs often used in laptops and mobile devices will typically share one set of RAM with the CPU. This reality can make things complicated for a graphics programmer, since you potentially have two different physical pools of &ldquo;GPU memory&rdquo; that will have different characterstics depending on how you&rsquo;re using it. Earlier APIs such as D3D11 and OpenGL <em>sort of</em> exposed these pools by abstracting away GPU memory altogether behind different resource types, but in D3D12 and Vulkan you are now more-or-less directly exposed to lower-level allocation and management of GPU memory. Altogether it&rsquo;s complicated enough to cause confusion and headaches for beginners or even intermediate graphics programmers, and it tends to come up rather early since you usually need to grapple with it before you can create your first texture.</p>
<p>In this article we&rsquo;re going to dive in on this topic, and in particular cover the following things:</p>
<ul>
<li>The basics of GPU memory</li>
<li>How GPU memory works in D3D12</li>
<li>Common patterns in D3D12</li>
<li>Some timing results gathered from a D3D12 test app</li>
</ul>
<p>Ultimately I&rsquo;m going to cover a lot of things that were already covered in some form by <a href="https://asawicki.info/">Adam Sawicki</a>&rsquo;s <a href="https://www.youtube.com/watch?v=ML0YC77bSOc">excellent talk</a> from Digital Dragons 2021 about optimizing for GPU memory pools. I would recommend watching that talk either way, but I&rsquo;m hoping that this article can complement that presentation by adding some extra details as well as some real-world benchmark results.</p>
<p>To keep this article somewhat focused, I&rsquo;m also only going to talk about D3D12 here and ignore other APIs. I would suggest reading <a href="https://asawicki.info/news_1740_vulkan_memory_types_on_pc_and_how_to_use_them">Adam&rsquo;s post about Vulkan memory types</a> if you&rsquo;re looking for information specific to Vulkan. You can also check out his <a href="https://asawicki.info/news_1755_untangling_direct3d_12_memory_heap_types_and_pools">later post</a> about D3D12 memory types and pools for a great overview of how memory pools are exposed through the D3D12 API.</p>
<h2 id="basics-of-gpu-memory">Basics of GPU Memory<a href="#basics-of-gpu-memory" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>As I briefly mentioned in the intro, a particular GPU may or may not have its own physically separate pool of memory to use for storing whatever data it needs to access. In general this sort of &ldquo;split&rdquo; memory pools is something you see on the discrete GPUs found on video cards, whereas the integrated GPUs that are built onto the same die as the CPU will typically use the same physical memory as the CPU. To start, let&rsquo;s look at the latter case which is arguably simpler.</p>
<h3 id="integrateduma-gpus">Integrated/UMA GPUs<a href="#integrateduma-gpus" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>So-called &ldquo;integrated&rdquo; GPUs are what you&rsquo;ll find built into many of the CPU offerings from Intel as well as the APU offerings from AMD, and are called that because the GPU is literally integrated into the same die as the CPU:</p>
<p><img src="/images/gpu-memory-pools/intel_integrated_die.jpg" alt="Intel Skylake die shot with integrated GPU"></p>
<p>These types of GPU are not a monolith, especially if you look at what you might see in phones, consoles, and Apple/Mac devices. However if you&rsquo;re looking at the Windows/PC world you can usually assume that the following is true:</p>
<ul>
<li>The CPU and the GPU share the same physical memory, and often the same memory controller</li>
<li>The RAM being used is of the DDR variety typically used for CPUs, as opposed to the more exotic GDDR chips found in video cards</li>
<li>The amount of raw bandwidth available to the GPU is perhaps an order of magnitude lower than what you might find on high-end dedicated graphics cards. As an example, the <a href="https://en.wikipedia.org/wiki/List_of_Intel_graphics_processing_units#Gen12">Gen12 GPUs</a> found in recent Tiger Lake and Alder Lake CPUs from Intel will range from about ~50 GB/s to ~76 GB/s. Meanwhile an <a href="https://en.wikipedia.org/wiki/Radeon_RX_6000_series#Desktop">AMD RX 6800</a> can achieve 512 GB/s to its dedicated VRAM, while an <a href="https://en.wikipedia.org/wiki/GeForce_30_series">Nvidia RTX 3090</a> peaks at a whopping 936 GB/s!</li>
</ul>
<p>Since the CPU and GPU share the same pool of physical memory, this setup is often referred to as a &ldquo;Unified Memory Architecture&rdquo;, or UMA for short. While it might seem at first that this is purely a disadvantage relative to a faster dedicated memory pool, this isn&rsquo;t entirely true. Sharing the memory pool and mechanisms for accessing that memory can make it easier for the CPU and GPU to work together: both processors can directly read and write to the same physical memory addresses, and perhaps even have a shared level of cache hierarchy. This can improve performance for CPU/GPU communication, and also remove certain cases of bugs. The other main advantage is that the system and its applications are free to partition the single pool of memory however they see fit, without having to instead manage two separate pools of memory that can only be used for a single processor.</p>
<h3 id="dedicatednuma-gpus">Dedicated/NUMA GPUs<a href="#dedicatednuma-gpus" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>For a PC that has its own dedicated video card, the situation is of course very different: these video cards have their own bank of physical RAM chips right on the board. Since this RAM pool is physically separate from the CPU RAM, this setup is usually referred to as a &ldquo;Non-Unifed Memory Architecture&rdquo; or &ldquo;NUMA&rdquo; for short. Often this separate pool is accessed through its own memory controller that&rsquo;s present on the GPU, and can even have its own separate page tables and virtual address space.</p>
<p>As we mentioned before, dedicated VRAM is capable of achieiving significantly higher bandwidth than what you&rsquo;ll see for the DDR modules used by PC CPUs. The secret weapon here for VRAM is that they typically exotic GDDR RAM chips that are specifically designed for high-bandwidth use cases, and these chips are then wired onto very wide 256-bit or even 384-bit buses. The chips, bus, and the GPU itself are all explicitly designed to handle the sort of wide bursts of coalesced memory accesses that you can expect from the 32-wide SIMD arrays that shader programs execute on.</p>
<p><img src="/images/gpu-memory-pools/numa.png" alt=""></p>
<p>Since the memory pools are physically split on dedicated GPU setup, sharing data between the CPU and GPU is naturally much more complicated. For sharing to happen, everything needs to happen on the PCI-express bus that the video card is connected to. The GPU is capable of reading from CPU memory (SysRAM) over this bus, which it can do at around 16 GB/s for <a href="https://en.wikipedia.org/wiki/PCI_Express#PCI_Express_4.0">PCI Express 4.0</a> and around 32 GB/s with the newer PCI Express 5.0. 16 GB/s really isn&rsquo;t very much compared with the GPU&rsquo;s bandwidth to VRAM or even the CPU&rsquo;s bandwidth to SysRAM, which makes it a resource that has to be utilized rather carefully. A video card will usually have a dedicated DMA unit on board that is optimized for asynchronously &ldquo;uploading&rdquo; large chunks of texture or buffer data from SysRAM to VRAM. PCIe can also service general texture and buffer reads issued by the shader core, which can work out well for small buffers that are updated frequently and fit comfortably in caches. The CPU also has the ability to directly write to VRAM over the PCIe bus, through a mechanism known as &ldquo;<a href="https://en.wikipedia.org/wiki/PCI_configuration_space#Bus_enumeration">Base Address Register</a>&rdquo;, or BAR for short. Until recently the amount of VRAM accessible through BAR was limited to around 256 MB, limiting its usefulness to special case data that was carefully managed by drivers. More modern systems however can support a feature known as &ldquo;<a href="https://www.nvidia.com/en-us/geforce/news/geforce-rtx-30-series-resizable-bar-support/">Resizable BAR</a>&rdquo;, or ReBAR, which can make the entire pool of VRAM accessible from the CPU.</p>
<p>The fact that the CPU and GPU have their own memory controllers and cache hierarchies can cause further pain and complexity for CPU/GPU communication. PC video cards can typically <a href="https://en.wikipedia.org/wiki/Bus_snooping">snoop</a> the caches of the CPU when the CPU writes to GPU-visible SysRAM, however CPU and GPU cache hierarchies are still effectively seperate. This leads to something of a dilemma for the CPU when writing to memory that only the GPU will read from: does it make sense to leave caching enabled and &ldquo;pollute&rdquo; the cache with data from GPU, or would it make more sense to bypass the CPU cache entirely? The latter has historically been the default on PC, while making use of a hardware feature known as <a href="https://en.wikipedia.org/wiki/Write_combining">write combining</a>. This feature effectively buffers nearby writes together so that each individual write doesn&rsquo;t need its own separate uncached bus transaction, which can work decently as long as you&rsquo;re doing fairly boring memcpy-style sequential writes. However this does nothing at all for you if you are unlucky enough to try to read from GPU-visible memory: the reads are completely uncached, and they will be <a href="https://fgiesen.wordpress.com/2013/01/29/write-combining-is-not-your-friend/">quite slow</a>. Things are even worse if you somehow read from VRAM over PCIe, which can be catostrophically slow. Enabling caching can avoid the performance penalty from accidental reads, however it may cause adverse performance for subsequent code by pushing important CPU data out of cache.</p>
<h2 id="how-it-works-in-d3d12">How It Works In D3D12<a href="#how-it-works-in-d3d12" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>D3D12 applications have to allocate their own memory, so how is all of this exposed to us? Adam Sawicki already gave a great overview of how everything works on <a href="https://asawicki.info/news_1755_untangling_direct3d_12_memory_heap_types_and_pools">his blog</a>, and there is also information in the <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/memory-management">official programming guide for D3D12</a>. For completeness I&rsquo;ll provide a brief overview here as well, along with some additional information based on my own experiences.</p>
<p>In D3D12 you have two different ways to allocate GPU-accessible memory for your device: by <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createcommittedresource">creating a committed resource</a>, or by <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createheap">creating a heap</a>. Heaps are the more direct way of allocating and utilizing memory, where you must create a heap first with various properties and then &ldquo;place&rdquo; resources within that heap via <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource">CreatePlacedResource</a>. Committed resources allocate memory indirectly: it&rsquo;s effectively the same as if the runtime/driver created a single resource-sized heap for you behind the scenes, and then placed the resource in that heap for you. For that reason, we will focus on heaps here instead of committed resources.</p>
<p>Once you&rsquo;ve created your device, you can query some feature bits in order to get some information in advance about the architecture and behavior of the GPU that you will be allocating memory for. In particular there are two fields of interest on <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/ns-d3d12-d3d12_feature_data_architecture1">D3D12_FEATURE_DATA_ARCHITECTURE1</a>, which you obtain by calling <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-checkfeaturesupport">CheckFeatureSupport</a> with <code>D3D12_FEATURE_ARCHITECTURE1</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="n">D3D12_FEATURE_DATA_ARCHITECTURE1</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">UINT</span> <span class="n">NodeIndex</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">BOOL</span> <span class="n">TileBasedRenderer</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">BOOL</span> <span class="n">UMA</span><span class="p">;</span>                 <span class="c1">// &lt; -- 1
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">BOOL</span> <span class="n">CacheCoherentUMA</span><span class="p">;</span>    <span class="c1">// &lt; -- 2
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">BOOL</span> <span class="n">IsolatedMMU</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">D3D12_FEATURE_DATA_ARCHITECTURE1</span><span class="p">;</span>
</span></span></code></pre></div><ol>
<li>The <code>UMA</code> flag tells if the GPU uses a Unified Memory Architecture where a single physical pool of RAM is shared between the CPU and GPU, like what you&rsquo;d normally find on an integrated GPU</li>
<li>If <code>UMA</code> is true and <code>CacheCoherentUMA</code> is also true, then the CPU and GPU caches are fully coherent with each other. In this situation using uncached writes for filling GPU-visible SysRAM does not make sense, since the GPU can benefit from data being resident in cache.</li>
</ol>
<p>In a bit we&rsquo;ll talk about how these flags affect the behavior of a device and ways an app can respond to them, but first let&rsquo;s look at the lowest-level mechanism for creating a heap: creating a &ldquo;custom&rdquo; heap. Custom heaps allow the app to fully specify all of the supported attributes of a heap, which is done through these two structs:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="n">D3D12_HEAP_DESC</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">UINT64</span> <span class="n">SizeInBytes</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_HEAP_PROPERTIES</span> <span class="n">Properties</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">UINT64</span> <span class="n">Alignment</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_HEAP_FLAGS</span> <span class="n">Flags</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">D3D12_HEAP_DESC</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">struct</span> <span class="n">D3D12_HEAP_PROPERTIES</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_HEAP_TYPE</span> <span class="n">Type</span><span class="p">;</span>   <span class="c1">// Must be D3D12_HEAP_TYPE_CUSTOM
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">D3D12_CPU_PAGE_PROPERTY</span> <span class="n">CPUPageProperty</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_MEMORY_POOL</span> <span class="n">MemoryPoolPreference</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">UINT</span> <span class="n">CreationNodeMask</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="n">UINT</span> <span class="n">VisibleNodeMask</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">D3D12_HEAP_PROPERTIES</span><span class="p">;</span>
</span></span></code></pre></div><p>First, there is <code>SizeInBytes</code> which is exactly what it sounds like. There is also an <code>Alignment</code> needed for satisfying alignment requirements for specific kinds of resources (in particular, MSAA textures). There is also a set of <code>D3D12_HEAP_FLAGS</code> that you can configure, but they are not generally relevant to our focus on memory pools and so we will ignore this. Instead, let&rsquo;s look at two of the interesting enums that you can specify in <code>D3D12_HEAP_PROPERTIES</code>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="n">D3D12_MEMORY_POOL</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_MEMORY_POOL_UNKNOWN</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1">// Not relevant for custom heaps
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">D3D12_MEMORY_POOL_L0</span>  <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_MEMORY_POOL_L1</span>  <span class="o">=</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">D3D12_MEMORY_POOL</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">typedef</span> <span class="k">enum</span> <span class="n">D3D12_CPU_PAGE_PROPERTY</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_CPU_PAGE_PROPERTY_UNKNOWN</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>    <span class="c1">// Not relevant for custom heaps
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">D3D12_CPU_PAGE_PROPERTY_NOT_AVAILABLE</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_COMBINE</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_BACK</span>  <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span> <span class="n">D3D12_CPU_PAGE_PROPERTY</span><span class="p">;</span>
</span></span></code></pre></div><p>Now things are getting interesting! <code>D3D12_MEMORY_POOL</code> is exactly what it sounds like: it allows us to choose the physical pool of RAM that the memory will live in. On NUMA devices (dedicated video cards) L0 maps to SysRAM AKA CPU memory, while L1 maps to VRAM located on the video card. However on UMA devices there is only physical memory pool available, which is LO. L1 is not available on such devices, and attempting to use it will cause the CreateHeap call to fail. This choice of memory pool must also be carefully paired with an appropriate <code>D3D12_CPU_PAGE_PROPERTY</code> in order to play nice with the CPU&rsquo;s ability (or lack thereof) to access the memory pool. In practice it ends up working something like this:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">if</span><span class="p">(</span><span class="n">UMA</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span><span class="p">(</span><span class="n">MemoryPoolPreference</span> <span class="o">==</span> <span class="n">D3D12_MEMORY_POOL_L0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Memory will live SysRAM pool shared between CPU and GPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_NOT_AVAILABLE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Not accessible to the CPU at all
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_COMBINE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Uncached for the CPU. Write combined for CPU writes, very very slow CPU reads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Ok for passing data from CPU -&gt; GPU.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_BACK</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span><span class="p">(</span><span class="n">CacheCoherentUMA</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="c1">// CPU pages are cached, fast for writes and reads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// GPU potentially benefits from data being in cache as well.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// Good for all purposes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">}</span>
</span></span><span class="line"><span class="cl">            <span class="k">else</span>
</span></span><span class="line"><span class="cl">            <span class="p">{</span>
</span></span><span class="line"><span class="cl">                <span class="c1">// Cached for CPU. May not have ideal CPU performance when
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// compared with uncached/write-combined. Good for
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>                <span class="c1">// reading back data from GPU -&gt; CPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">MemoryPoolPreference</span> <span class="o">==</span> <span class="n">D3D12_MEMORY_POOL_L1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Error, can&#39;t use L1 on UMA!
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="k">else</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="c1">// NUMA device
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">MemoryPoolPreference</span> <span class="o">==</span> <span class="n">D3D12_MEMORY_POOL_L0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Memory will live in the SysRAM pool
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_NOT_AVAILABLE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Lives in SysRAM but the CPU can&#39;t access it, don&#39;t do this!
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_COMBINE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Uncached for the CPU. Write combined for CPU writes, very very slow CPU reads.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// GPU will read this data over PCIe at low speeds relative to VRAM.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Good for passing data from CPU -&gt; GPU
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_BACK</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Cached for the CPU with full speed reads and writes.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// GPU will access this data over PCIe at low speeds relative to VRAM.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Good for reading back data from GPU -&gt; CPU.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Can be used for CPU -&gt; GPU traffic as well, but may pollute
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// the CPU cache with data that will only be read by the GPU.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span> <span class="nf">if</span><span class="p">(</span><span class="n">MemoryPoolPreference</span> <span class="o">==</span> <span class="n">D3D12_MEMORY_POOL_L1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="c1">// Memory will live in the high-bandwidth VRAM pool
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="k">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_NOT_AVAILABLE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// The CPU can&#39;t access this memory at all, only the GPU can.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Good for data that only the GPU will ever read or write, such as render targets.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// Good for persistent, read-only resources but filling the data requires
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>            <span class="c1">// a two-step upload process.
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="k">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_COMBINE</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Error, not valid for a NUMA device
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span> <span class="k">if</span><span class="p">(</span><span class="n">CPUPageProperty</span> <span class="o">==</span> <span class="n">D3D12_CPU_PAGE_PROPERTY_WRITE_BACK</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="p">{</span>
</span></span><span class="line"><span class="cl">            <span class="c1">// Error, not valid for a NUMA device
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>        <span class="p">}</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>This is naturally a bit complicated to manage all of this yourself, so to make things a but easier D3D12 provides &ldquo;built-in&rdquo; heap types that are optimized for common use cases. These types are accessed by using the <code>D3D12_HEAP_TYPE_DEFAULT</code>, <code>D3D12_HEAP_TYPE_UPLOAD</code>, or <code>D3D12_HEAP_TYPE_READBACK</code> values of <code>D3D12_HEAP_TYPE</code> when creating the heap, and setting the <code>MemoryPoolPreference</code> to <code>D3D12_MEMORY_POOL_UNKNOWN</code> and <code>CPUPageProperty</code> to <code>D3D12_CPU_PAGE_PROPERTY_UNKNOWN</code>. Here&rsquo;s how they work:</p>
<ul>
<li><code>DEFAULT</code> is intended for resources that the GPU primarily accesses at full bandwidth. On NUMA this maps to L1 with no CPU access, while on UMA it maps to L0 with no CPU access.</li>
<li><code>UPLOAD</code> is intended for CPU -&gt; GPU communication where the CPU writes the data and the GPU reads it. On NUMA this maps to L0 with WRITE_COMBINE CPU access, while on UMA it maps to L0 with WRITE_COMBINE if CacheCoherentUMA is false, and WRITE_BACK if CacheCoherentUMA is true.</li>
<li><code>READBACK</code> is intended for GPU -&gt; CPU communication where the GPU writes the data and the CPU reads it. On both UMA and NUMA devices this maps to L0 with WRITE_BACK CPU access.</li>
</ul>
<p>One thing you may have noticed about this setup is that CPU-accessible VRAM is not currently exposed through any of these options. While the driver can use the small amount of BAR VRAM if it decides it&rsquo;s worthwhile, applications have no direct way of allocating from that special bit of memory. This is true even on newer machines with ReBAR support that makes the entire pool of VRAM accessible to the CPU. However, here is an NVAPI extension that allows you to <a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/coresdk/nvapi/group__dx.html#gac0e1d0851b25d2d956b13eaa64752bf9">query</a> and <a href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/coresdk/nvapi/group__dx.html#ga1c24c3f5a5c6bf9dd3a35b660b4b95db">allocate</a> from CPU-accessible VRAM if the device was created on a supported Nvidia GPU. Meanwhile, AMD&rsquo;s driver can automatically promote allocations to CPU-accessible VRAM based on heuristics and ReBAR support. Hopefully a future version of D3D12 adds support for this functionality, since it is certainly a useful option.</p>
<p>It&rsquo;s not necessarily obvious from reading the official documentation, but these various built-in and custom heap types come with limitations on resource usage that are either implicit or explicit depending on the exact use case. In general buffer resources are not limited: you can place them in any of the built-in heap types as well as any possible configuration of a custom heap, provided that the heap wasn&rsquo;t created with <code>D3D12_HEAP_FLAG_DENY_BUFFERS</code>. Buffers have an implied linear layout of the data that is similar to an array in C/C++, which makes it trivial for both CPU and GPU to access these resources. Textures however are special: these typically do <em>not</em> use a linear layout<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, but instead use a hardware-specific swizzle pattern that&rsquo;s designed for maximum spatial locality and cache efficiency. This isn&rsquo;t an issue if the GPU accesses the texture since it knows how to read/write to these special patterns, but it&rsquo;s a major issue if you wanted to access such a texture from the CPU: your app doesn&rsquo;t know the swizzle pattern, which means only the driver knows how to encode or decode it. Even if you could access the texture you likely wouldn&rsquo;t want to read it from SysRAM on a NUMA device, since the slow PCIe bandwidth could degrade performance. The built-in heaps use a heavy-handed means of guarding you from these issues by flat-out forbidding you from creating texture resources in an UPLOAD heap. Instead you must use a &ldquo;two-step&rdquo; upload scheme where the CPU writes un-swizzled data to a buffer in UPLOAD memory, and then the GPU itself <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12graphicscommandlist-copybufferregion">copies and swizzles</a> the data to a texture resource in DEFAULT memory.</p>
<p>With custom heaps, you can potentially bypass the texture/heap limitations but it must be done carefully. D3D12 will let you place a ROW_MAJOR texture resource in a CPU-writable L0 heap even on a NUMA device, which you can then fill from the CPU. But you should be prepared for excruciatingly poor performance if you try to do this, due poor spatial locality coupled with slow PCIe bandiwdth. A somewhat better option is to use the special <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12resource-writetosubresource">WriteToSubresource</a> function, which allows the driver to fill an UNKNOWN-layout texture using a hardware-specific swizzle pattern. This would likely still be too slow on NUMA hardware that reads from L0, however it can be compelling option for UMA devices that share a memory pool anyway. WriteToSubresource allows you to forego a GPU-side copy to initialize your resources, which is appealing since integrated GPUs often lack an asynchronous DMA for performing said copies. WriteToSubresource might one day be relevant for dedicated video cards if ReBAR becomes the norm an D3D12 exposes CPU-accessible VRAM, however it ultimately may not be desirable for apps to burn a CPU core on swizzling texture data.</p>
<p>If you want to keep an eye on how much memory your app is using from the two pools, <a href="https://docs.microsoft.com/en-us/windows/win32/api/dxgi1_4/nf-dxgi1_4-idxgiadapter3-queryvideomemoryinfo">QueryVideoMemoryInfo</a> can be called from a <code>IDXGIAdapter3</code> to get the current usage of L0 and L1. To do this you just need to know that <code>DXGI_MEMORY_SEGMENT_GROUP_LOCAL</code> maps to L1, while <code>DXGI_MEMORY_SEGMENT_GROUP_NON_LOCAL</code> maps to L0. The OS-assigned budgets returned by this function are also useful to keep an eye on. In particular if you start to execeed the L1 budget, the OS can start to automatically &ldquo;demote&rdquo; some of your heaps from L1 to L0 in order to leave enough free VRAM for other processes on the system. If this demotion happens to the wrong resource it can destroy your performance, and so you really want to avoid that situation if you can help it. <a href="https://devblogs.microsoft.com/pix/system-monitor/">PIX for Windows</a> can also show you this information when attached to your app, which is handy. If demotion does occur you can mitigate the damage by <a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device1-setresidencypriority">assigning a residency priority</a> on your heaps/resources. This can let you keep your more critical resources in VRAM by allowing the OS to demote less-important things.</p>
<h2 id="common-patterns-in-d3d12">Common Patterns in D3D12<a href="#common-patterns-in-d3d12" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>After discussing some of the specifics of the API, let&rsquo;s now talk about how D3D12 apps and samples typically work with memory pools.</p>
<h3 id="textures-and-the-two-step-upload">Textures And The Two-Step Upload<a href="#textures-and-the-two-step-upload" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>To start, let&rsquo;s discuss textures since they are somewhat special. As we covered earlier, textures tend to be swizzled in an opaque hardware-specific layout while also being quite large and bandwidth-hungry. Therefore the common choice is to throw these guys in a DEFAULT heap, which maps to the high-bandwidth L1/VRAM pool on NUMA devices (and as we mentioned earlier, the runtime won&rsquo;t even let you create a texture in an UPLOAD heap). This puts us in our desired physical memory pool on both UMA and NUMA devices, but introduces a separate problem: the CPU cannot directly initialize the contents of a texture living in DEFAULT. Instead we must rely on what I call the <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/upload-and-readback-of-texture-data">two-step upload process</a>, which goes something like this:</p>
<ol>
<li>Some memory from a buffer resource living in CPU-accessible memory (typically a UPLOAD heap) needs to be reserved, enough to fit the texture data being uploaded. This is sometimes called a &ldquo;staging&rdquo; buffer.</li>
<li>The CPU loads or copies un-swizzled texture data into the CPU-accessible buffer</li>
<li><a href="https://docs.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12graphicscommandlist-copytextureregion">CopyTextureRegion</a> is called on a command list, one for each subresource in the texture (mip level or array slice)</li>
<li>The command list is submitted on a queue where it executes on the GPU, and proper synchronization is used to ensure tha the copy completes before another command attempts to read from the texture</li>
</ol>
<p>This is naturally a bit tricky since you have a few tough choices to make in terms of exactly you do this. In particular you need to choose how exactly you allocate/aquire the UPLOAD buffer used as the source of the copy, and you also need to decide how and when you submit your command list on a command queue. The official D3D12 samples use a very simple scheme where an UPLOAD buffer is created as a committed resource and the copy is immediately submitted on a DIRECT command queue (via <a href="https://github.com/microsoft/DirectX-Headers/blob/main/include/directx/d3dx12.h#L2210">UpdateSubresources</a> from the d3dx12.h helper header) followed by an immediate CPU wait for the submission to finish. That&rsquo;s totally fine for simple use cases, but what if you have many uploads to do and you don&rsquo;t want to spend a ton of time creating and destroying committed resources? What if you need to stream in these textures in background while most of your CPU cores are busy and your GPU is already executing giant batches of rendering work on the DIRECT queue? What if there&rsquo;s a texture that needs to be updated every single frame, and you don&rsquo;t want updating it to stall out one of your rendering threads? For these situations you probably want a more complex approach that better fits the use case.</p>
<h3 id="working-with-the-copy-queue">Working With The COPY Queue<a href="#working-with-the-copy-queue" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>In many cases it&rsquo;s ideal to use the dedicated COPY queue type to perform this kind of uploading. On NUMA devices this queue typically feeds to a dedicated DMA unit on the video card, which is specifically optimized for perform asynchronous copies of data over the PCIe bus. This not only means it may be more efficient than a copy performed on a DIRECT queue, but it also means that the actual copy operation can potentially run in parallel to other graphics and compute work running on the GPU! For first-time initialization this is not a big deal, but for background streaming/loading it&rsquo;s definitely a better option than forcing your critical per-frame drawing to wait for a bunch of uploads to finish executing (which is what will happen if you submit your copies to your main DIRECT queue). Async copies can still slow down your graphics work a bit by consuming some bandwidth, but this is generally still a better proposition than forcing your graphics work to wait for a bunch of slow PCIe copies to finish. To illustrate the difference, the images below are from two different GPU Trace captures taken with Nvidia Nsight graphics:</p>
<p><a href="/images/gpu-memory-pools/buffer_upload_direct_queue.png"><img src="/images/gpu-memory-pools/buffer_upload_direct_queue.png" alt=""></a></p>
<p><a href="/images/gpu-memory-pools/buffer_upload_copy_queue.png"><img src="/images/gpu-memory-pools/buffer_upload_copy_queue.png" alt=""></a></p>
<p>The top image is capture where a 64 MB buffer is uploaded every frame from SysRAM to VRAM using CopyBufferRegion on the main DIRECT queue, which is followed by a compute dispatch that immediately reads from the uploaded buffer. The bottom image shows what things look like if the CopyBufferRegion is instead submitted to COPY queue, allowing it to overlap with the previous frame&rsquo;s work on the DIRECT queue. Notice how the frame time is 3.5ms shorter when using the COPY queue, despite the dispatch taking about 10% longer when overlapped with the upload.</p>
<p>The trade-off here is that you will take on some significant complexity by using a separate COPY queue instead of re-using your primary queue. Submitting on an async queue means it is now up to you to properly synchronize your submissions across multiple queues: you will need to signal a fence on your COPY queue after submitting a command list and then have another queue wait on that fence before commands on the latter queue can read from the resources being uploaded. This is exactly like &ldquo;async compute&rdquo; setups that are common for overlapping compute work with graphics work, and has some of the same headaches. Like async compute, you need to carefully manage when exactly you submit work and sync points on your queues if you&rsquo;re hoping for them to overlap: if you want your uploads to overlap some graphics work, you can&rsquo;t just submit to both at the same time with a fence wait in between, since that will just turn into synchronous/blocking copy! Instead you generally want to submit your uploads well in advance of when you need them, or more accurately when you tell your DIRECT queue to wait on them. You also need to take care not to delete or otherwise re-use your staging buffers until the CPU has fully waited for the COPY fence to be signaled, otherwise you will get incorrect results and/or a GPU crash.</p>
<p>One extra bonus of using a COPY queue is that under the existing (and <a href="https://microsoft.github.io/DirectX-Specs/d3d/D3D12EnhancedBarriers.html">soon-to-be-legacy</a>) barrier setup there are <a href="https://docs.microsoft.com/en-us/windows/win32/direct3d12/using-resource-barriers-to-synchronize-resource-states-in-direct3d-12#state-decay-to-common">special rules</a> carved out for resource decay and promotion when a COPY queue is involved. When a resource is copied to or from on a COPY queue, that resource automatically decays back to a COMMON state after the COPY command list finished executing. Since COMMON resources get auto-promoted on their first use, this means that you can typically completely skip issuing any barriers for read-only resources that you initialize this way! This extremely convenient, and can even save some performance as well.</p>
<p>Before moving on, I should point out that all of the above really only applies for NUMA/dedicated devices. A UMA/integrated device often doesn&rsquo;t have a dedicated DMA engine, and so any submissions to COPY queues will end up getting <a href="../breaking-down-barriers-part-5-back-to-the-real-world/#the-present-windows-10-d3d12-and-wddm-20">serialized/flattened</a> onto a single hardware queue. The typical DEFAULT/UPLOAD setup will still work correctly on ths type of hardware, but if you want to optimize for integrated devices then you may want to consider adding a path that uses custom heaps to allow the CPU to directly initialize your textures.</p>
<h3 id="allocating-staging-memory">Allocating Staging Memory<a href="#allocating-staging-memory" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>As I alluded to earlier, another complicating aspect of the two-step upload is that you need to reserve a sufficient amount of staging memory from <em>somewhere</em>. This memory then needs to remain allocated and untouched until the copying queue completely finishes executing the submission that pulls from the staging buffer. The simple approach seen in some samples is to create a committed buffer resource every time some data needs to be uploaded, and then destroy it after the submission is finished. While this certainly works, creating those committed resoures is definitely not free (especially if you omit the <code>D3D12_HEAP_FLAG_CREATE_NOT_ZEROED </code> flag!), and you could slow down your resource loading and creation by constantly creating and freeing like that. Another tricky issue is memory usage: if you keep allocating more staging buffers, you won&rsquo;t be able to free them until the GPU consumes them. This can result in unbounded memory usage if many uploads are being batched into a single queue submission during engine initialization. A single big submission with tons of uploads can also be bad on integrated GPUs that don&rsquo;t have an async DMA unit that you can use, in which case you&rsquo;re just going to stall the main graphics queue until all of the uploads finish.</p>
<p>As an alternative, one might consider sub-allocating from a single large staging buffer instead of constantly creating new ones. Since the allocation is tied to the GPU progress of copy operations, a ring buffer can be a pretty natural fit. The overall flow could go something like this:</p>
<ol>
<li>An upload is requested for N bytes</li>
<li>Check if there is enough space in the ring buffer, if not stall and wait for previous submissions to finish</li>
<li>Allocate space in the ring buffer by moving the tail back N bytes</li>
<li>Copy resource data into the ring buffer at the right offset</li>
<li>Record a copy command on a command list</li>
<li>Submit the command list on a queue</li>
<li>Tell the queue to signal a fence once the command list finishes executing</li>
<li>Periodically check the fence value of previous submissions, if they&rsquo;ve completed move up the ring buffer head to free up more space</li>
<li>If the ring buffer is full and somebody needs to upload, wait on the submission fence until enough space frees up</li>
</ol>
<p>This is but one possibility for an allocation scheme, and even within such a setup there are more choices to be made. For instance, should it be thread-safe? If multiple threads can use it, which operations can happen in parallel? Is the calling code responsible for copying into the staging buffer and issuing copy commands, or is the uploader mechanism responsible? When should you submit to the GPU, and when should you poll for completed submissions? Should you batch multiple uploads into a single submission? How does this interact with the &ldquo;main&rdquo; per-frame submissions on DIRECT and COMPUTE queues? How big should your ring buffer be, and what should happen if an upload exceeds the size of that ring buffer? These questions don&rsquo;t necessarily have one-size-fits all answer, so it really depends on your engine and use case.</p>
<p>One example of the ring buffer setup can be found in <a href="https://github.com/TheRealMJP/DXRPathTracer/blob/sm_6_6/SampleFramework12/v1.02/Graphics/DX12_Upload.cpp">my sample framework</a>. It uses locks to ensure thread-safety by serializing allocations from the ring buffer as well as submissions to the COPY queue, but it allows multiple calling threads to copy into staging memory and record copy commands in parallel once they&rsquo;ve successfully allocated. It also allows for uploads to happen on background streaming threads that are decoupled from frame rendering. This is achieved by having a 1:1 relationship between uploads and submissions, which is definitely a non-optimal choice for uploading many small resources in succession. In that case the uploads can become bottlenecked by the max number of submissions rather than the ring buffer itself, and overhead from queue submission could start to dominate. The async COPY queue submissions are also waited on by the next frame&rsquo;s DIRECT/COMPUTE queue submissions, which is always safe but usually too conservative.</p>
<p>For another possible example, check out <a href="https://github.com/microsoft/DirectXTK12/wiki/ResourceUploadBatch">ResourceUploadBatch</a> from DirectXTK. This helper class still allocates a committed resource for use as staging memory, but allows (manually) batching multiple uploads together into a single submission. It also makes use of <code>std::future</code> as a way of tracking when an upload actually completes, which is a nice touch.</p>
<h3 id="what-about-buffers">What About Buffers?<a href="#what-about-buffers" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>I was primarlity talking about textures in terms of the two-step upload, but what about buffers? Don&rsquo;t they need to get uploaded too? The answer to that is &ldquo;yes, sometimes&rdquo;. For large, read-only or rarely-updated buffers allocating them in VRAM and uploading their contents makes a ton of sense, and you can likely use the same mechanisms that you would use for initializing textures. The situation is slightly easier since you don&rsquo;t need to worry about CopyTextureRegion and RowPitch alignment requirements, but is otherwise mostly the same.</p>
<p>Things are different though for smaller, frequently-updated buffers. For example, a 64-byte constant buffer containing a per-draw world transform that you update every frame. In these cases the data can fit nicely in cache, so you&rsquo;re probably not losing very much performance (if any) by letting the GPU read it over PCIe. Also if you&rsquo;re only reading the constant buffer once, you&rsquo;re liable to actually make performance <em>worse</em> by copying form SysRAM to VRAM first. Therefore you can usually save yourself the headache for these kinds of small dynamic buffers, and just let them live in SysRAM. If the data is truly fire-and-forget and doesn&rsquo;t need to persist longer than the current frame being recorded, you can use really fast linear allocator patterns to <a href="https://github.com/TheRealMJP/DXRPathTracer/blob/sm_6_6/SampleFramework12/v1.02/Graphics/DX12_Upload.cpp#L448">quickly grab a bit of temporary UPLOAD memory</a> that&rsquo;s tied to frame lifetime.</p>
<p>Where things get trickier is buffers that are not small, are updated fairly frequently, and are accessed either more than once or with non-uniform addresses. For these cases you probably do want the full bandwidth of VRAM, but uploading them every single frame is perhaps not appealing if your uploader is tuned more for background resource loading/initialization. In these cases I&rsquo;ve found you may want a sort-of hybrid approach: you want to quickly grab a bit of staging/UPLOAD memory that only needs to persist for the current frame, and then you want feed that into a batched COPY queue upload that&rsquo;s submitted once a frame. This gives you a fast path for getting the buffer data into VRAM that won&rsquo;t hold up critical rendering threads.</p>
<p>Once CPU-accessible VRAM is ubiquitous through ReBAR, it seems plausible that nearly all CPU-updated buffers could live in VRAM and be directly updated from the CPU. This would simplify the situation considerably compared to what&rsquo;s required now. For integrated UMA GPUs that situation already exists: there&rsquo;s really no reason to do a two-step upload for buffer resources on these systems, and it would benefit engines to detect this case if they want to optimize for these GPUs.</p>
<h3 id="what-about-directstorage">What About DirectStorage?<a href="#what-about-directstorage" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>Some of you may have been asking yourself &ldquo;hey, how does DirectStorage fit into this?&rdquo; as you were reading through this section. DirectStorage is capable of fully managing the full journey from disk to staging memory to VRAM all on its own, which is pretty awesome. Unfortunately I&rsquo;m not personally familar with the inner details of how DirectStorage manages these things, nor do I have much experience with using it. I would assume they are biasing for high throughput of upload operations, since the main goal of that API is to improve loading times. Based on <a href="https://github.com/microsoft/DirectStorage/blob/main/Docs/DeveloperGuidance.md#staging-buffers-and-copying">the docs</a>, it sounds like they always suballocate staging memory from a single upload buffer and the operation will fail if the request is larger than the configured staging buffer size.</p>
<h2 id="results-from-my-testing-app">Results From My Testing App<a href="#results-from-my-testing-app" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>While it&rsquo;s useful to look at things in terms of theoretical max bandwidth, it&rsquo;s helpful to gather some actual performance numbers from real working hardware. To that end, I created simple D3D12 application that can and gather timing data from a synthetic performance test. The test itself is nothing fancy: a buffer has its contents updated from the CPU, and then a compute shader is dispatched to read from that buffer. There are options to specify what kind of heap the buffer is allocated from, the size of the buffer, the number of compute shader threads, and the access pattern used by those threads. By timing the CPU update, we can gather some data indicating how quickly or slowly the CPU can fill the buffer with different memory pools and caching protocols. Optionally, we can also time how long it takes to <em>read</em> from the buffer in order to get an idea of how badly this will destroy performance. When the app is run, it presents some real-time results along with a simple interface that looks like this:</p>
<p><img src="/images/gpu-memory-pools/testing-app.png" alt=""></p>
<p>In the compute shader itself, a starting address is computed for each thread. The shader then runs an N-long loop, where each iteration reads and sums a single 16-byte element of the source buffer. The settings for controlling how the addresses are calculated are a bit obtuse, but together they allow setting up some commonly-seen access patterns to <a href="https://github.com/sebbbi/perftest">see how they fare with different memory and buffer types</a>. The shader itself is quite short so I&rsquo;ll post it here for reference:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="k">const</span> <span class="n">uint</span> <span class="n">ThreadGroupSize</span> <span class="o">=</span> <span class="mi">256</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="n">numthreads</span><span class="p">(</span><span class="n">ThreadGroupSize</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]</span>
</span></span><span class="line"><span class="cl"><span class="kt">void</span> <span class="n">ComputeJob</span><span class="p">(</span><span class="n">in</span> <span class="n">uint3</span> <span class="nl">dispatchThreadID</span> <span class="p">:</span> <span class="n">SV_DispatchThreadID</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">in</span> <span class="n">uint3</span> <span class="nl">groupThreadID</span> <span class="p">:</span> <span class="n">SV_GroupThreadID</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                <span class="n">in</span> <span class="n">uint3</span> <span class="nl">groupID</span> <span class="p">:</span> <span class="n">SV_GroupID</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">uint</span> <span class="n">localThreadIndex</span> <span class="o">=</span> <span class="n">groupThreadID</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">uint</span> <span class="n">globalThreadIndex</span> <span class="o">=</span> <span class="n">dispatchThreadID</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">const</span> <span class="n">uint</span> <span class="n">groupIndex</span> <span class="o">=</span> <span class="n">groupID</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="cp">#if RawBuffer_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="n">ByteAddressBuffer</span> <span class="n">inputBuffer</span> <span class="o">=</span> <span class="n">RawBufferTable</span><span class="p">[</span><span class="n">AppSettings</span><span class="p">.</span><span class="n">InputBufferIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="cp">#elif FormattedBuffer_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="n">Buffer</span><span class="o">&lt;</span><span class="n">float4</span><span class="o">&gt;</span> <span class="n">inputBuffer</span> <span class="o">=</span> <span class="n">InputBuffers</span><span class="p">[</span><span class="n">AppSettings</span><span class="p">.</span><span class="n">InputBufferIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="cp">#elif StructuredBuffer_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>        <span class="n">StructuredBuffer</span><span class="o">&lt;</span><span class="n">float4</span><span class="o">&gt;</span> <span class="n">inputBuffer</span> <span class="o">=</span> <span class="n">InputBuffers</span><span class="p">[</span><span class="n">AppSettings</span><span class="p">.</span><span class="n">InputBufferIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">    <span class="kt">float</span> <span class="n">sum</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Values with a trailing underscore are compile-time constants, allowing full
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// loop unrolling and dispatch-uniform/wave-uniform buffer access where applicable
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="n">uint</span> <span class="n">elemIdx</span> <span class="o">=</span> <span class="p">(</span><span class="n">ElemsPerThread_</span> <span class="o">*</span> <span class="n">ThreadGroupSize</span> <span class="o">*</span> <span class="n">GroupElemOffset_</span> <span class="o">*</span> <span class="n">groupIndex</span><span class="p">)</span> <span class="o">+</span>
</span></span><span class="line"><span class="cl">                    <span class="p">(</span><span class="n">ElemsPerThread_</span> <span class="o">*</span> <span class="n">ThreadElemOffset_</span> <span class="o">*</span> <span class="n">localThreadIndex</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span><span class="p">(</span><span class="n">uint</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">ElemsPerThread_</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="k">const</span> <span class="n">uint</span> <span class="n">loadIdx</span> <span class="o">=</span> <span class="n">elemIdx</span> <span class="o">%</span> <span class="n">NumInputBufferElems_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="cp">#if RawBuffer_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>            <span class="k">const</span> <span class="n">float4</span> <span class="n">data</span> <span class="o">=</span> <span class="n">asfloat</span><span class="p">(</span><span class="n">inputBuffer</span><span class="p">.</span><span class="n">Load4</span><span class="p">(</span><span class="n">loadIdx</span> <span class="o">*</span> <span class="mi">16</span><span class="p">));</span>
</span></span><span class="line"><span class="cl">        <span class="cp">#elif StructuredBuffer_ || FormattedBuffer_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>            <span class="k">const</span> <span class="n">float4</span> <span class="n">data</span> <span class="o">=</span> <span class="n">inputBuffer</span><span class="p">[</span><span class="n">loadIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="cp">#elif ConstantBuffer_
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>            <span class="k">const</span> <span class="n">float4</span> <span class="n">data</span> <span class="o">=</span> <span class="n">CBuffer</span><span class="p">.</span><span class="n">Elems</span><span class="p">[</span><span class="n">loadIdx</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">        <span class="cp">#endif
</span></span></span><span class="line"><span class="cl"><span class="cp"></span>
</span></span><span class="line"><span class="cl">        <span class="n">sum</span> <span class="o">+=</span> <span class="n">dot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mf">1.0f</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">elemIdx</span> <span class="o">+=</span> <span class="n">ThreadElemStride_</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1">// Should not actually execute, this is just here to keep the
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="c1">// compiler from optimizing away the entire shader
</span></span></span><span class="line"><span class="cl"><span class="c1"></span>    <span class="k">if</span><span class="p">(</span><span class="n">sum</span> <span class="o">&lt;</span> <span class="mf">0.0f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">OutputBuffer</span><span class="p">.</span><span class="n">Store</span><span class="p">(</span><span class="n">globalThreadIndex</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="n">sum</span><span class="p">);</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>I&rsquo;ve used my desktop PC for gathering the data below, which has a Nvidia RTX 2080 with an AMD Ryzen 9 5900X CPU. The GPU has a peak bandwidth of 448 GB/s to VRAM, and as a PCIe 4.0 part it has a max bandwidth of about 16 GB/s for accessing memory over that bus. Meanwhile the CPU is connected to 32 GB of DDR4 3600, which has a peak bandwidth of around 28 GB/s.</p>
<h3 id="cpu-write-performance">CPU Write Performance<a href="#cpu-write-performance" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<div>
    <canvas id="cpu-write-chart" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/cpu-write-chart.js"></script>
</div>
<p>Here we&rsquo;re measuring the time it took for a single 5900X core to fill 16 MB buffer. For this test I only used CPU-accessible memory, since non-CPU-accessible VRAM requires additional work to schedule a GPU copy for the upload. What we see here is that the uncached and cached paths are basically identical, meaning that write combining is doing its job for our simple access pattern we&rsquo;re using (which is just a memcpy). For the special CPU-accessible VRAM path available through NVAPI, the time grows by about 40% reflecting the fact that we are writing over a slow PCI express bus. For SysRAM this works out to about 15.5 GB/s, while for VRAM it&rsquo;s about 11.6 GB/s.</p>
<h3 id="cpu-read-performance">CPU Read Performance<a href="#cpu-read-performance" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<div>
    <canvas id="cpu-read-chart" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/cpu-read-chart.js"></script>
</div>
<p>For this test case, we want to see how quickly (or slowly) the CPU is able to read from CPU-visible GPU memory. For uncached SysRAM it takes about 4ms to read the entire 16 MB buffer, which is about 4x more time than it took to fill the buffer with write combining. Meanwhile for cached SysRAM it&rsquo;s about 60x faster, no doubt because the buffer is still present in the cache hierarchy after being filled just prior to the reading step. As for CPU-visible VRAM, we have officially entered &ldquo;oh no, what I have done?&rdquo; territory at a whopping 26.66ms. Remember kids: friends don&rsquo;t let friends read from VRAM on the CPU.</p>
<h3 id="gpu-read-performance-normal-access">GPU Read Performance, Normal Access<a href="#gpu-read-performance-normal-access" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<div>
    <canvas id="gpu-read-chart" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/gpu-read-chart.js"></script>
</div>
<p>For this test, we are now looking at GPU performance by measuring how long it takes for the GPU to complete the single big compute shader dispatch that reads from the buffer. While this doesn&rsquo;t serve as a comprehensive bandwidth or cache performance test, it tells us roughly the overall effect that the GPU memory pool will have on GPU performance which is primarily what we are interested in. To gather the data, the test app was configured to use a 64 MB buffer and launch 16 * 1024 * 256 = 4M threads that each access a single contiguous 16-byte element from the input buffer. This effectively reads from the entire buffer, giving us a rough idea of low long it takes to read the whole thing.</p>
<p>As you would expect for a dedicated GPU, there&rsquo;s a significant disparity between reading from SysRAM (L0) and VRAM (L1). Reading from SysRAM causes the compute shader to take about ~32x longer to execute relative to VRAM, reflecting the significantly lower theoretical bandwidth for reading over PCIe vs VRAM. This is very close to what we would expect given that my RTX 2080 is rated at 448 GB/s for bandwidth to VRAM, and PCIe 4.0 tops out at 16 GB/s.</p>
<p>To help confirm things, let&rsquo;s invert these numbers to estimate what the minimum required bandwidth would be in order for the dispatch to complete in the reported time:</p>
<div>
    <canvas id="gpu-read-bandwidth-chart" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/gpu-read-bandwidth-chart.js"></script>
</div>
<p>While our calculated numbers fall a bit short of the expected theoretical bandwidth, this is probably expected given that our shader may not be hitting the exact conditions required for achieving peak bandwidth on the hardware. However they still clearly reflect the extreme bandwidth deficit that&rsquo;s experienced when reading from SysRAM.</p>
<h3 id="gpu-read-performance-non-coalesced-access">GPU Read Performance, Non-Coalesced Access<a href="#gpu-read-performance-non-coalesced-access" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<div>
    <canvas id="gpu-read-non-coalesced-chart" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/gpu-read-non-coalesced-chart.js"></script>
</div>
<p>For the previous test, each thread accessed a buffer element that was exactly contiguous with the element being accessed by neighboring threads. GPUs can typically accelerate this case by issuing <a href="https://gpuopen.com/learn/gcn-memory-coalescing/">coalesced memory requests</a>. This pattern is also good in general for cache efficiency: all data from a cache line ends up getting utilized, as opposed to unused neighboring data getting inadvertently pulled into cache. In this test we use a less efficient access pattern where each thread reads a 16 byte element that is 64 bytes away from what their neighboring threads read. If the cache line is 64 bytes, this means that 48 of those 64 bytes are wasted which will slow things down considerably. This is roughly reflected in the test numbers, where the dispatch is about 3-4x slower for all memory types when compared with the previous test case.</p>
<h3 id="gpu-read-performance-various-buffer-sizes">GPU Read Performance, Various Buffer Sizes<a href="#gpu-read-performance-various-buffer-sizes" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h3>
<p>Earlier in the article we discussed how the <em>size</em> of the buffer being read is something that should factor into our decision of whether to place the buffer in VRAM or SysRAM. The general rule of thumb is &ldquo;small&rdquo; buffers are ok being in SysRAM, particularly if they are only read once and/or many threads end up reading the same data from that buffer (constant buffers typically fit this pattern). Let&rsquo;s try to verify that assumption by running a test where we run a smaller dispatch of 1M threads that access various sizes of smaller buffers. The setup is such that each thread will mod their address with the buffer size, effectively causing the reads to &ldquo;wrap around&rdquo;. Therefore if 1M threads each reads a 16-byte element from the input buffer and that buffer is 512 KB, we would expect each byte of that buffer to be read (16 * 1024 * 1024) / (512 * 1024) = 32 times. Let&rsquo;s now have a look at the results:</p>
<div>
    <canvas id="gpu-read-sizes-chart" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/gpu-read-sizes-chart.js"></script>
</div>
<p>At the leftmost end of the chart we see that the gap between SysRAM and VRAM is almost 0 for a tiny 64 KB buffer. This suggests that at typical constant buffer sizes there is little benefit to VRAM, and the speed of the GPU&rsquo;s cache hierarchies will dominate. As we increase in size though see the SysRAM times pull away from the VRAM times, the latter essentially appearing to have a flat slope at these scales. This suggests that the limited bandwidth of PCIe is bottlenecking the SysRAM dispatch more and more as the buffer size increases, but for VRAM we are likely bottlenecked elsewhere (perhaps on the output writes). Let&rsquo;s zoom out a bit more and see what we get for 1 to 16 MB buffer sizes:</p>
<div>
    <canvas id="gpu-read-sizes-chart2" width="" height=""></canvas>
    <script src="/js/gpu-memory-pools/gpu-read-sizes-2-chart.js"></script>
</div>
<p>Up to 4 MB we continue our trend of SysRAM pulling away from the VRAM results, with SysRAM being about 10x slower at that point. But then at 5 MB things jump way up to 1.285 milliseconds, and it stays up there! What gives? What&rsquo;s so special about 4 MB that things take a nosedive after that point? Well it turns out that 4 MB is the size of the L2 cache on my RTX 2080! Essentially at &lt;= 4 MB the entire buffer was able to fit in L2 cache, which meant that the dispatch time roughly reflected the amount of time it would require to fill that &lt;= 4 MB from memory. If you compute the minimum required bandwidth (buffer size divided by execution time) for these &lt; 4 MB timings, they come out to about 10 GB/s which is fairly close to the 12.5 GB/s I measured from SysRAM in other tests. Meanwhile for the &gt;= 5 MB data, that 1.285 millisecond number that repeats for each test is exactly what you get if you calculate how long it takes to read a 16 MB buffer with 12.5 GB/s bandwidth. 16 MB is what you get if you sum the size of all reads from every thread regardless of the buffer size (1M threads, each reading 16 bytes), which means we&rsquo;re basically pegged at 12.5 GB/s &ldquo;effective&rdquo; bandwidth for these measurements. Since our data set no longer fits comfortably in L2 cache, we can&rsquo;t count on the data to still be in cache the Nth time an element is accessed by a later thread.</p>
<h2 id="conclusion">Conclusion<a href="#conclusion" class="anchor" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M15 7h3a5 5 0 0 1 5 5 5 5 0 0 1-5 5h-3m-6 0H6a5 5 0 0 1-5-5 5 5 0 0 1 5-5h3"></path><line x1="8" y1="12" x2="16" y2="12"></line></svg></a></h2>
<p>Having to manage separate physical memory pools for GPU programming can be rather complicated, and there are definitely performance pitfalls that we need to watch out for. Integrated GPUs that share a memory pool with the CPU can be simpler, but we still need to interact with them using APIs that accomodate dedicated video cards. In most cases we also need to run the same code on both dedicated and integrated GPUs, which means we need to try to handle both cases gracefully. To minimize the pain, I would recommend the following high-level practices:</p>
<ul>
<li>For dedicated video cards:
<ul>
<li>Make sure you&rsquo;re using VRAM for all textures as well as buffers that do not need to be updated from the CPU. The bandwidth difference between SysRAM and VRAM can be nearly 100x on high-end video cards.</li>
<li>Try to use a COPY/Transfer queue to initialize VRAM resources asynchronously to your graphics work.</li>
<li>For larger buffers that need to be updated from the CPU, consider allocating them in VRAM and using a staging buffer + COPY/Transfer queue to update them.</li>
<li>On APIs and systems that support it (Vulkan and NVAPI), consider allocating large CPU-updated buffers in CPU-writable VRAM so that you can update them directly. Watch out for potentially slower CPU write performance. <em><strong>Do not read ever from this memory on the CPU!</strong></em></li>
</ul>
</li>
<li>For integrated/mobile GPUs:
<ul>
<li>Skip using the COPY/transfer queue to initialize or update buffers, you can just make them CPU-writable with no GPU performance penalty.</li>
<li>With D3D12 you can still use DEFAULT heaps if you like, but it&rsquo;s still going to be located in SysRAM.</li>
</ul>
</li>
<li>In general:
<ul>
<li>Be careful not to read from memory that is uncached for the CPU, it can be quite slow! Try to stick to a simple memcpy when updating GPU memory from the CPU.</li>
</ul>
</li>
</ul>
<p>That&rsquo;s all for now, good luck with wrangling those GPUs and their memory pools!</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>D3D12 actually provides two different ways for you to create a texture with a &ldquo;known&rdquo; layout that you can encode/decode from the CPU. The first is <code>D3D12_TEXTURE_LAYOUT_ROW_MAJOR</code>, which uses a standard linear layout that is very inefficient for the GPU to access due to lack of 2D locality. The second is through <code>D3D12_TEXTURE_LAYOUT_64KB_STANDARD_SWIZZLE</code>, which uses a known standardized swizzle pattern that&rsquo;s common across hardware. While the standard swizzle sounds like a nice option, in practice Intel/AMD/Nvidia do not actually support it on their hardware. This is likely because the standardized pattern is less efficient than the hardware-specific patterns currently in use.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

			</div>
			<hr class="post-end">
			<footer class="post-info">
				<p>
					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://therealmjp.github.io/tags/graphics">Graphics</a></span><span class="tag"><a href="https://therealmjp.github.io/tags/dx12">DX12</a></span>
				</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>8195 Words</p>
				<p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2022-07-24 17:00 -0700</p>
			</footer>
		</article>
		<div class="post-nav thin">
		</div>
		<div id="comments" class="thin"><script src="https://utteranc.es/client.js"
        repo="TheRealMJP/TheRealMJP.github.io"
        issue-term="title"
        label="comments"
        theme="photon-dark"
        crossorigin="anonymous"
        async>
</script></div>
	</main>

	<footer id="site-footer" class="section-inner thin animated fadeIn faster">
		<p>&copy; 2022 <a href="https://therealmjp.github.io/">MJP</a></p>
		<p>
			Made with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> &#183; Theme <a href="https://github.com/Track3/hermit" target="_blank" rel="noopener">Hermit</a> &#183; <a href="https://therealmjp.github.io/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
		</p>
	</footer>


	<script src="https://therealmjp.github.io/js/main.min.6e4a8d6406e68b5f99eb4fd82c6e3eaa5aa471527d2d301aceaecdfefdd04bc9.js" integrity="sha256-bkqNZAbmi1+Z60/YLG4+qlqkcVJ9LTAazq7N/v3QS8k="></script>



</body>

</html>
