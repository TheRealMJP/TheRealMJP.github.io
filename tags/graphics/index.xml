<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Graphics on The Danger Zone</title>
    <link>https://therealmjp.github.io/tags/graphics/</link>
    <description>Recent content in Graphics on The Danger Zone</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 22 Aug 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://therealmjp.github.io/tags/graphics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Read My Chapter in Ray Tracing Gems II!</title>
      <link>https://therealmjp.github.io/posts/rtg2-bindless/</link>
      <pubDate>Sun, 22 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/rtg2-bindless/</guid>
      <description>Ray Tracing Gems II was released a few weeks ago, and like the first one it&#39;s free to download as a PDF from the publisher&#39;s website. There are 50 articles in this thing written by over 80 authors, so I think it&#39;s safe to say that this book is absolutely jam-packed with useful knowledge and techniques related to ray tracing.
Yours truly contributed a chapter called &amp;quot;Using Bindless Resources With DirectX Raytracing&amp;quot;, which as you&#39;d expect is all about using bindless techniques in the context of DXR.</description>
    </item>
    
    <item>
      <title>Approximating Subsurface Scattering With Spherical Gaussians</title>
      <link>https://therealmjp.github.io/posts/sss-sg/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sss-sg/</guid>
      <description>In my previous post, where I gave a quick overview of real-time subsurface scattering techniques, I mentioned that one of the potential downsides of preintegrated subsurface scattering was that it relied on sampling a precomputed lookup texture for storing the appropriate &amp;quot;wrapped&amp;quot; or &amp;quot;blurred&amp;quot; lighting response for a given lighting angle and surface curvature. Depending on the hardware you&#39;re working with as well as the exact details of the shader program, sampling a lookup texture can be either a good or a bad thing.</description>
    </item>
    
    <item>
      <title>An Introduction To Real-Time Subsurface Scattering</title>
      <link>https://therealmjp.github.io/posts/sss-intro/</link>
      <pubDate>Sun, 06 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sss-intro/</guid>
      <description>A little while ago I was doing some research into the state-of-the-art for approximating subsurface scattering effects in real-time (mainly for skin rendering), and I had taken a bunch of loose notes to help me keep all of the details straight. I thought it might be useful to turn those notes into a full blog post, in case anyone else out there needs an overview of what&#39;s commonly used to shade skin and other materials in recent games.</description>
    </item>
    
    <item>
      <title>Half The Precision, Twice The Fun: Working With FP16 In HLSL</title>
      <link>https://therealmjp.github.io/posts/shader-fp16/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/shader-fp16/</guid>
      <description>Those of you who have been working on desktop and console graphics long enough will remember working with fp16 math in shaders during the D3D9 era. Back then HLSL supported the half scalar type, which corresponded to a floating-point value using 16-bits of precision. Using it was crucial for extracting the best performance from Nvidia&#39;s FX series, 6-series, and 7-series hardware, since it could perform many fp16 operations at faster rate than it could for full-precision 32-bit values.</description>
    </item>
    
    <item>
      <title>Breaking Down Barriers - Part 6: Experimenting With Overlap and Preemption</title>
      <link>https://therealmjp.github.io/posts/breaking-down-barriers-part-6-experimenting-with-overlap-and-preemption/</link>
      <pubDate>Mon, 10 Dec 2018 02:01:27 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/breaking-down-barriers-part-6-experimenting-with-overlap-and-preemption/</guid>
      <description>This is Part 6 of a series about GPU synchronization and preemption. You can find the other articles here:
Part 1 - What&amp;rsquo;s a Barrier?
Part 2 - Synchronizing GPU Threads
Part 3 - Multiple Command Processors
Part 4 - GPU Preemption
Part 5 - Back To The Real World
Part 6 - Experimenting With Overlap and Preemption
In the previous art_icl_es we took a look at how barriers typically work on GPUs, and then we examined how multiple hardware queues can help with preemption and overall throughput.</description>
    </item>
    
    <item>
      <title>Breaking Down Barriers – Part 5: Back To The Real World</title>
      <link>https://therealmjp.github.io/posts/breaking-down-barriers-part-5-back-to-the-real-world/</link>
      <pubDate>Sun, 09 Sep 2018 00:48:18 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/breaking-down-barriers-part-5-back-to-the-real-world/</guid>
      <description>This is Part 5 of a series about GPU synchronization and preemption. You can find the other articles here:
Part 1 - What&amp;rsquo;s a Barrier?
Part 2 - Synchronizing GPU Threads
Part 3 - Multiple Command Processors
Part 4 - GPU Preemption
Part 5 - Back To The Real World
Part 6 - Experimenting With Overlap and Preemption
Welcome to part 5 of the series! If you&amp;rsquo;ve read all of the articles so far, thanks for hanging in there!</description>
    </item>
    
    <item>
      <title>Breaking Down Barriers - Part 4: GPU Preemption</title>
      <link>https://therealmjp.github.io/posts/breaking-down-barriers-part-4-gpu-preemption/</link>
      <pubDate>Wed, 04 Jul 2018 00:57:43 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/breaking-down-barriers-part-4-gpu-preemption/</guid>
      <description>This is Part 4 of a series about GPU synchronization and preemption. You can find the other articles here:
Part 1 - What&amp;rsquo;s a Barrier?
Part 2 - Synchronizing GPU Threads
Part 3 - Multiple Command Processors
Part 4 - GPU Preemption</description>
    </item>
    
    <item>
      <title>Breaking Down Barriers - Part 3: Multiple Command Processors</title>
      <link>https://therealmjp.github.io/posts/breaking-down-barriers-part-3-multiple-command-processors/</link>
      <pubDate>Mon, 18 Jun 2018 02:14:52 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/breaking-down-barriers-part-3-multiple-command-processors/</guid>
      <description>This is Part 3 of a series about GPU synchronization and preemption. You can find the other articles here:
Part 1 - What&amp;rsquo;s a Barrier?
Part 2 - Synchronizing GPU Threads
Part 3 - Multiple Command Processors
Part 4 - GPU Preemption</description>
    </item>
    
    <item>
      <title>Breaking Down Barriers - Part 2: Synchronizing GPU Threads</title>
      <link>https://therealmjp.github.io/posts/breaking-down-barriers-part-2-synchronizing-gpu-threads/</link>
      <pubDate>Mon, 02 Apr 2018 06:29:17 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/breaking-down-barriers-part-2-synchronizing-gpu-threads/</guid>
      <description>This is Part 2 of a series about GPU synchronization and preemption. You can find the other articles here:
Part 1 - What&amp;rsquo;s a Barrier?
Part 2 - Synchronizing GPU Threads
Part 3 - Multiple Command Processors
Part 4 - GPU Preemption</description>
    </item>
    
    <item>
      <title>Breaking Down Barriers - Part 1: What&#39;s a Barrier?</title>
      <link>https://therealmjp.github.io/posts/breaking-down-barriers-part-1-whats-a-barrier/</link>
      <pubDate>Tue, 06 Mar 2018 09:21:34 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/breaking-down-barriers-part-1-whats-a-barrier/</guid>
      <description>This is Part 1 of a series about GPU synchronization and preemption. You can find the other articles here:
Part 1 - What&amp;rsquo;s a Barrier?
Part 2 - Synchronizing GPU Threads
Part 3 - Multiple Command Processors
Part 4 - GPU Preemption
Part 5 - Back To The Real World
Part 6 - Experimenting With Overlap and Preemption
If you&amp;rsquo;ve done any amount of D3D12 or Vulkan programming, then you&amp;rsquo;ve probably spent a good bit of that time grappling with barriers.</description>
    </item>
    
    <item>
      <title>SG Series Part 6: Step Into The Baking Lab</title>
      <link>https://therealmjp.github.io/posts/sg-series-part-6-step-into-the-baking-lab/</link>
      <pubDate>Mon, 10 Oct 2016 07:13:58 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sg-series-part-6-step-into-the-baking-lab/</guid>
      <description>This is part 6 of a series on Spherical Gaussians and their applications for pre-computed lighting. You can find the other articles here:
Part 1 - A Brief (and Incomplete) History of Baked Lighting Representations
Part 2 - Spherical Gaussians 101
Part 3 - Diffuse Lighting From an SG Light Source
Part 4 - Specular Lighting From an SG Light Source
Part 5 - Approximating Radiance and Irradiance With SG&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>SG Series Part 5: Approximating Radiance and Irradiance With SG&#39;s</title>
      <link>https://therealmjp.github.io/posts/sg-series-part-5-approximating-radiance-and-irradiance-with-sgs/</link>
      <pubDate>Mon, 10 Oct 2016 07:12:13 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sg-series-part-5-approximating-radiance-and-irradiance-with-sgs/</guid>
      <description>This is part 5 of a series on Spherical Gaussians and their applications for pre-computed lighting. You can find the other articles here:
Part 1 - A Brief (and Incomplete) History of Baked Lighting Representations
Part 2 - Spherical Gaussians 101
Part 3 - Diffuse Lighting From an SG Light Source
Part 4 - Specular Lighting From an SG Light Source
Part 5 - Approximating Radiance and Irradiance With SG&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>SG Series Part 4: Specular Lighting From an SG Light Source</title>
      <link>https://therealmjp.github.io/posts/sg-series-part-4-specular-lighting-from-an-sg-light-source/</link>
      <pubDate>Mon, 10 Oct 2016 07:09:20 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sg-series-part-4-specular-lighting-from-an-sg-light-source/</guid>
      <description>This is part 4 of a series on Spherical Gaussians and their applications for pre-computed lighting. You can find the other articles here:
Part 1 - A Brief (and Incomplete) History of Baked Lighting Representations
Part 2 - Spherical Gaussians 101
Part 3 - Diffuse Lighting From an SG Light Source
Part 4 - Specular Lighting From an SG Light Source
Part 5 - Approximating Radiance and Irradiance With SG&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>SG Series Part 3: Diffuse Lighting From an SG Light Source</title>
      <link>https://therealmjp.github.io/posts/sg-series-part-3-diffuse-lighting-from-an-sg-light-source/</link>
      <pubDate>Mon, 10 Oct 2016 07:08:51 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sg-series-part-3-diffuse-lighting-from-an-sg-light-source/</guid>
      <description>This is part 3 of a series on Spherical Gaussians and their applications for pre-computed lighting. You can find the other articles here:
Part 1 - A Brief (and Incomplete) History of Baked Lighting Representations
Part 2 - Spherical Gaussians 101
Part 3 - Diffuse Lighting From an SG Light Source
Part 4 - Specular Lighting From an SG Light Source
Part 5 - Approximating Radiance and Irradiance With SG&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>SG Series Part 2: Spherical Gaussians 101</title>
      <link>https://therealmjp.github.io/posts/sg-series-part-2-spherical-gaussians-101/</link>
      <pubDate>Mon, 10 Oct 2016 07:08:02 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sg-series-part-2-spherical-gaussians-101/</guid>
      <description>This is part 2 of a series on Spherical Gaussians and their applications for pre-computed lighting. You can find the other articles here:
Part 1 - A Brief (and Incomplete) History of Baked Lighting Representations
Part 2 - Spherical Gaussians 101
Part 3 - Diffuse Lighting From an SG Light Source
Part 4 - Specular Lighting From an SG Light Source
Part 5 - Approximating Radiance and Irradiance With SG&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>SG Series Part 1: A Brief (and Incomplete) History of Baked Lighting Representations</title>
      <link>https://therealmjp.github.io/posts/sg-series-part-1-a-brief-and-incomplete-history-of-baked-lighting-representations/</link>
      <pubDate>Mon, 10 Oct 2016 07:05:49 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/sg-series-part-1-a-brief-and-incomplete-history-of-baked-lighting-representations/</guid>
      <description>This is part 1 of a series on Spherical Gaussians and their applications for pre-computed lighting. You can find the other articles here:
Part 1 - A Brief (and Incomplete) History of Baked Lighting Representations
Part 2 - Spherical Gaussians 101
Part 3 - Diffuse Lighting From an SG Light Source
Part 4 - Specular Lighting From an SG Light Source
Part 5 - Approximating Radiance and Irradiance With SG&amp;rsquo;s</description>
    </item>
    
    <item>
      <title>New Blog Series: Lightmap Baking and Spherical Gaussians</title>
      <link>https://therealmjp.github.io/posts/new-blog-series-lightmap-baking-and-spherical-gaussians/</link>
      <pubDate>Mon, 10 Oct 2016 07:05:10 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/new-blog-series-lightmap-baking-and-spherical-gaussians/</guid>
      <description>So nearly year and a half ago myself and Dave Neubelt gave a presentation at SIGGRAPH where we described the approach that we developed for approximating incoming radiance using Spherical Gaussians in both our lightmaps and 3D probe grids. We had planned on releasing a source code demo as well as course notes that would serve as a full set of implementation details, but unfortunately those efforts were sidetracked by other responsibilities.</description>
    </item>
    
    <item>
      <title>Bindless Texturing for Deferred Rendering and Decals</title>
      <link>https://therealmjp.github.io/posts/bindless-texturing-for-deferred-rendering-and-decals/</link>
      <pubDate>Fri, 25 Mar 2016 08:39:36 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/bindless-texturing-for-deferred-rendering-and-decals/</guid>
      <description>https://github.com/TheRealMJP/DeferredTexturing
https://github.com/TheRealMJP/DeferredTexturing/releases (Precompiled Binaries)
To Bind, or Not To Bind Unless you&amp;rsquo;ve been in a coma for the past year, you&amp;rsquo;ve probably noticed that there&amp;rsquo;s a lot of buzz and excitement around the new graphics API&amp;rsquo;s that are available for PC and mobile. One of the biggest changes brought by both D3D12 and Vulkan is that they&amp;rsquo;ve ditched the old slot-based system for binding resources that&amp;rsquo;s been in use since&amp;hellip;forever. In place of the old system, both API&amp;rsquo;s have a adopted a new model[1] based around placing opaque resource descriptors in contiguous ranges of GPU-accessible memory.</description>
    </item>
    
    <item>
      <title>Mitsuba Quick-Start Guide</title>
      <link>https://therealmjp.github.io/posts/mitsuba-quick-start-guide/</link>
      <pubDate>Sat, 04 Apr 2015 20:36:44 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/mitsuba-quick-start-guide/</guid>
      <description>Angelo Pesce&amp;rsquo;s recent blog post brought up a great point towards the end of the article: having a &amp;ldquo;ground-truth&amp;rdquo; for comparison can be extremely important for evaluating your real-time techniques. For approximations like pre-integrated environment maps it can help visualize what kind of effect your approximation errors will have on a final rendered image, and and in many other cases it can aid you in tracking down bugs in your implementation.</description>
    </item>
    
    <item>
      <title>Shadow Sample Update</title>
      <link>https://therealmjp.github.io/posts/shadow-sample-update/</link>
      <pubDate>Wed, 18 Feb 2015 18:00:06 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/shadow-sample-update/</guid>
      <description>Update 1/24/2016: one of the authors of the Moment Shadow Mapping paper contacted to let me know that there was an issue in my implementation of the 16-bit variant of EVSM. My sample app was clamping the maximum exponential warp factor to 10.0, which can result in overflow for a 16-bit float. This has the effect of reducing light bleeding, but it also causes edge quality to suffer during filtering. This made the light bleeding comparison with MSM16 unfair, particularly since my comparisons did not use high filtering settings.</description>
    </item>
    
    <item>
      <title>Weighted Blended Order-Independent Transparency</title>
      <link>https://therealmjp.github.io/posts/weighted-blended-oit/</link>
      <pubDate>Tue, 04 Feb 2014 06:58:04 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/weighted-blended-oit/</guid>
      <description>http://mynameismjp.files.wordpress.com/2014/02/blendedoit.zip
Back in December, Morgan McGuire and Louis Bavoil published a paper called Weighted Blended Order-Independent Transparency. In case you haven&amp;rsquo;t read it yet (you really should!), it proposes an OIT scheme that uses a weighted blend of all surfaces that overlap a given pixel. In other words finalColor = w0 * c0 + w1 * c1 + w2 * c2&amp;hellip;etc. With a weighted blend the order of rendering no longer matters, which frees you from the never-ending nightmare of sorting.</description>
    </item>
    
    <item>
      <title>A Sampling of Shadow Techniques</title>
      <link>https://therealmjp.github.io/posts/shadow-maps/</link>
      <pubDate>Wed, 11 Sep 2013 07:45:40 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/shadow-maps/</guid>
      <description>A little over a year ago I was looking to overhaul our shadow rendering at work in order to improve overall quality, as well as simplify the workflow for the lighting artists (tweaking biases all day isn&amp;rsquo;t fun for anybody). After doing yet another round of research into modern shadow mapping techniques, I decided to do what I usually do and starting working on sample project that I could use as a platform for experimentation and comparison.</description>
    </item>
    
    <item>
      <title>DX11.2 Tiled Resources</title>
      <link>https://therealmjp.github.io/posts/dx11-2-tiled-resources/</link>
      <pubDate>Sat, 07 Sep 2013 06:21:28 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/dx11-2-tiled-resources/</guid>
      <description>Tiled resources seems to be the big-ticket item for the upcoming DX11.2 update. While the online documentation has some information about the new functions added to the API, there&amp;rsquo;s currently no information about the two tiers of tiled resource functionality being offered. Fortunately there is a sample app available that provides some clues. After poking around a bit last night, these were the differences that I noticed:
  TIER2 supports MIN and MAX texture sampling modes that return the min or max of 4 neighboring texels.</description>
    </item>
    
    <item>
      <title>SIGGRAPH Follow-Up</title>
      <link>https://therealmjp.github.io/posts/siggraph-follow-up/</link>
      <pubDate>Mon, 29 Jul 2013 06:15:04 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/siggraph-follow-up/</guid>
      <description>So I&amp;rsquo;m hoping that if you&amp;rsquo;re reading this, you&amp;rsquo;ve already attended or read the slides from my presentation about The Order: 1886 that was part of the Physically Based Shading Course at SIGGRAPH last week. If not, go grab them and get started! If you haven&amp;rsquo;t read through the course notes already there&amp;rsquo;s a lot of good info there, in fact there&amp;rsquo;s almost 30 pages worth! The highlights include:
 Full description of our Cook-Torrance and Cloth BRDF&amp;rsquo;s, including a handy optimization for the GGX Smith geometry term (for which credit belongs to Steve McAuley) Analysis of our specular antialiasing solution Plenty of details regarding the material scanning process HLSL sample code for the Cook-Torrance BRDF&amp;rsquo;s as well as the specular AA roughness modification Lots of beautiful LaTeX equations  If you did attend, I really appreciate you coming and I hope that you found it interesting.</description>
    </item>
    
    <item>
      <title>Experimenting with Reconstruction Filters for MSAA Resolve</title>
      <link>https://therealmjp.github.io/posts/msaa-resolve-filters/</link>
      <pubDate>Mon, 29 Oct 2012 07:33:31 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/msaa-resolve-filters/</guid>
      <description>Previous article in the series: A Quick Overview of MSAA
Update 8/26/2017: while working on The Order I improved upon the work described here, which I presented at SIGGRAPH 2015. I also created an updated MSAA + TAA filtering demo that you can find on GitHub, which just about completely supersedes the demo that&amp;rsquo;s linked at the end of the article. So make sure that you look at the new one as well!</description>
    </item>
    
    <item>
      <title>A Quick Overview of MSAA</title>
      <link>https://therealmjp.github.io/posts/msaa-overview/</link>
      <pubDate>Thu, 25 Oct 2012 07:03:27 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/msaa-overview/</guid>
      <description>Previous article in the series: Applying Sampling Theory to Real-Time Graphics
Updated 1/27/2016 - replaced the MSAA partial coverage image with a new image that illustrates subsamples being written to, as suggested by Simon Trümpler.
MSAA can be a bit complicated, due to the fact that it affects nearly the entire rasterization pipeline used in GPU’s. It’s also complicated because really understanding why it works requires at least a basic understanding of signal processing and image resampling.</description>
    </item>
    
    <item>
      <title>Applying Sampling Theory To Real-Time Graphics</title>
      <link>https://therealmjp.github.io/posts/applying-sampling-theory-to-real-time-graphics/</link>
      <pubDate>Mon, 22 Oct 2012 06:59:09 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/applying-sampling-theory-to-real-time-graphics/</guid>
      <description>Previous article in the series: Signal Processing Primer
Computer graphics is a field that constantly deals with discrete sampling and reconstruction of signals, although you might not be aware of it yet. This article focuses on the ways in which sampling theory can be applied to some of the common tasks routinely performed in graphics and 3D rendering.
Image Scaling The concepts of sampling theory can are most easily applicable to graphics in the form of image scaling.</description>
    </item>
    
    <item>
      <title>Signal Processing Primer</title>
      <link>https://therealmjp.github.io/posts/signal-processing-primer/</link>
      <pubDate>Mon, 15 Oct 2012 08:20:18 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/signal-processing-primer/</guid>
      <description>For a theoretical understanding of aliasing and anti-aliasing, we can turn to the fields of signal processing[1] and sampling theory[2]. This article will explain some of the basics of these two related field in my own words, taking a more theoretical point of view. In the following article the concepts covered here will be used to analyze common aspects of real-time graphics, so that we can describe them in terms of signal processing.</description>
    </item>
    
    <item>
      <title>Upcoming Series on Signal Processing and MSAA</title>
      <link>https://therealmjp.github.io/posts/upcoming-series-on-signal-processing-and-msaa/</link>
      <pubDate>Mon, 15 Oct 2012 08:00:47 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/upcoming-series-on-signal-processing-and-msaa/</guid>
      <description>Aliasing is everywhere in graphics. Almost everything we do uses discrete sampling, which means almost everything can produce a variety of aliasing artifacts. The folks in the film industry have historically taken a “no aliasing allowed” stance in their work, but in real-time graphics we’re still producing games with more sparkling and shimmering than a glitzy prom dress. If we’re going to do anything about that problem, I think it’s important that we all try to have at least a basic understanding of signal processing.</description>
    </item>
    
    <item>
      <title>OpenGL Insights</title>
      <link>https://therealmjp.github.io/posts/opengl-insights/</link>
      <pubDate>Mon, 06 Aug 2012 04:50:49 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/opengl-insights/</guid>
      <description>Some time ago Charles de Rousiers adapted my Bokeh Depth of Field sample to OpenGL, and we contributed it as a chapter to the recently-released OpenGL Insights. Bokeh is still an ongoing area of R&amp;amp;D for myself, and hopefully I&amp;rsquo;ll be able to share some more improvements and optimizations once my current project is announced or released.
There&amp;rsquo;s going to be an author meet-up/book signing a the CRC Press SIGGRAPH booth (#929) this Tuesday from 2-3PM, and I&amp;rsquo;ll most likely be stopping by.</description>
    </item>
    
    <item>
      <title>Light Indexed Deferred Rendering</title>
      <link>https://therealmjp.github.io/posts/light-indexed-deferred-rendering/</link>
      <pubDate>Sun, 01 Apr 2012 02:53:53 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/light-indexed-deferred-rendering/</guid>
      <description>There&amp;rsquo;s been a bit of a stir on the Internet lately due to AMD&amp;rsquo;s recent Leo demo, which was recently revealed to be using a modern twist on Light Indexed Deferred Rendering. The idea of light indexed deferred has always been pretty appealing, since it gives you some of the advantages of deferred rendering (namely using the GPU to decide which lights affect each pixel) while still letting you use forward rendering to actually apply the lighting to each surface.</description>
    </item>
    
    <item>
      <title>10 Things That Need To Die For Next-Gen</title>
      <link>https://therealmjp.github.io/posts/things-that-need-to-die/</link>
      <pubDate>Tue, 06 Dec 2011 09:54:34 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/things-that-need-to-die/</guid>
      <description>Lately I&amp;rsquo;ve been thinking about things in graphics that have long worn out their welcome, and I started a list of techniques that I hope will be nowhere in sight once everyone moves on to next-gen console hardware (or starts truly exploiting high-end PC hardware). Here they are, in no particular order:
  Phong/Blinn-Phong - we need more expressive BRDF&amp;rsquo;s for our materials, and these guys are getting in the way.</description>
    </item>
    
    <item>
      <title>GPU Profiling in DX11 with Queries</title>
      <link>https://therealmjp.github.io/posts/profiling-in-dx11-with-queries/</link>
      <pubDate>Thu, 13 Oct 2011 08:59:37 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/profiling-in-dx11-with-queries/</guid>
      <description>For profiling GPU performance on the PC, there aren&amp;rsquo;t too many options. AMD&amp;rsquo;s GPU PerfStudio and Nvidia&amp;rsquo;s Parallel Nsight can be pretty handy due to their ability to query hardware performance counters and display the data, but they only work on each vendor&amp;rsquo;s respective hardware. You also might want to integrate some GPU performance numbers into your own internal profiling systems, in which case those tools aren&amp;rsquo;t going to be of much use.</description>
    </item>
    
    <item>
      <title>Average luminance calculation using a compute shader</title>
      <link>https://therealmjp.github.io/posts/average-luminance-compute-shader/</link>
      <pubDate>Wed, 10 Aug 2011 09:31:03 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/average-luminance-compute-shader/</guid>
      <description>A common part of most HDR rendering pipelines is some form of average luminance calculation. Typically it&amp;rsquo;s used to implement Reinhard&amp;rsquo;s method of image calibration, which is to map the geometric mean of luminance (log average) to some &amp;ldquo;key value&amp;rdquo;. This, combined with some time-based adaptation, allows for a reasonable approximation of auto-exposure or human eye adaptation.
In the old days of DX9, the average luminance calculation was usually done repeatedly downscaling a luminance texture as if generating mipmaps.</description>
    </item>
    
    <item>
      <title>I am officially a published author</title>
      <link>https://therealmjp.github.io/posts/i-am-officially-a-published-author/</link>
      <pubDate>Fri, 05 Aug 2011 06:04:11 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/i-am-officially-a-published-author/</guid>
      <description>I recently collaborated with fellow DX MVP&amp;rsquo;s Jason Zink and Jack Hoxley to write a D3D11-focused book entitled Practical Rendering and Computation with Direct3D 11, which just came up for sale on Amazon today. I wrote the HLSL and Deferred Rendering chapters in particular. All of the code samples are up on the Hieroglyph 3 CodePlex site, if you want to get an idea of the content. Or you can just take my word for it that it&amp;rsquo;s awesome.</description>
    </item>
    
    <item>
      <title>Anamorphic lens flares: the lens flare of the 2010&#39;s?</title>
      <link>https://therealmjp.github.io/posts/lens-flares/</link>
      <pubDate>Fri, 10 Jun 2011 06:57:41 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/lens-flares/</guid>
      <description>Since the dawn of time, the greatest struggle of the graphics programmer is to ensure that bright stuff looks really damn bright. We&amp;rsquo;re stuck with displays that have a limited displayable range, which means it&amp;rsquo;s fallen upon us to come up with new hacks and tricks to make sure the player at least feels like he&amp;rsquo;s blinded by the sun, even if we can&amp;rsquo;t really cause physical damage to their eyes (if only!</description>
    </item>
    
    <item>
      <title>Bokeh II: The Sequel</title>
      <link>https://therealmjp.github.io/posts/bokeh-ii-the-sequel/</link>
      <pubDate>Wed, 20 Apr 2011 06:59:20 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/bokeh-ii-the-sequel/</guid>
      <description>After I finished the bokeh sample, there were a few remaining issues that I wanted to tackle before I was ready to call it &amp;ldquo;totally awesome&amp;rdquo; and move on with my life.
Good blur - in the last sample I used either a 2-pass blur on a poisson disc performed at full resolution, or a bilateral Gaussian blur performed at 1/4 resolution (both done in a pixel shader). The former is nice because it gives you variable filter width per-pixel, but you get some ugly noise-like artifacts due to insufficient sampling.</description>
    </item>
    
    <item>
      <title>How To Fake Bokeh (And Make It Look Pretty Good)</title>
      <link>https://therealmjp.github.io/posts/bokeh/</link>
      <pubDate>Mon, 28 Feb 2011 08:18:35 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/bokeh/</guid>
      <description>Before I bought a decent DSLR camera and started putting it in manual mode, I never really noticed bokeh that much. I always just equated out-of-focus with blur, and that was that. But now that I&amp;rsquo;ve started noticing, I can&amp;rsquo;t stop seeing it everywhere. And now every time I see depth of field effects in a game that doesn&amp;rsquo;t have bokeh, it just looks wrong. A disc blur or even Gaussian blur is fine for approximating the look of out-0f-focus areas that are mostly low-frequency, but the hot spots just don&amp;rsquo;t look right at all (especially if you don&amp;rsquo;t do it in HDR).</description>
    </item>
    
    <item>
      <title>Radiosity, DX11 Style</title>
      <link>https://therealmjp.github.io/posts/radiosity-dx11-style/</link>
      <pubDate>Mon, 31 Jan 2011 08:08:09 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/radiosity-dx11-style/</guid>
      <description>Radiosity isn&amp;rsquo;t exactly new. According to Wikipedia it&amp;rsquo;s been used for rendering since the early 80&amp;rsquo;s, and this page looks like it may have been the first web page on the Internet. The basic premise is dead simple: for each point where you want to bake lighting (typically either a texel in a lightmap, or a vertex in a mesh), render the rest of the scene and any exterior light sources (skydome, area lights, sun, whatever) in all directions within a hemisphere surrounding the surface normal at that point.</description>
    </item>
    
    <item>
      <title>Position From Depth in GLSL</title>
      <link>https://therealmjp.github.io/posts/position-from-depth-glsl-style/</link>
      <pubDate>Sun, 09 Jan 2011 01:47:45 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/position-from-depth-glsl-style/</guid>
      <description>Commenter &amp;ldquo;Me&amp;rdquo; was kind enough to share his GLSL implementation of a deferred point light shader, which makes use of one of the methods I previously posted for reconstructing position from depth. So I figured I&amp;rsquo;d post it here, for all of you unfortunate enough to be stuck with writing shaders in GLSL. :P
// deferred shading VERTEX (GEOMETRY) varying vec3 normalv, posv; void main( void ) { normalv = ( gl_NormalMatrix * gl_Normal ).</description>
    </item>
    
    <item>
      <title>Conservative Depth Output (and Other Lesser-Known D3D11 Features)</title>
      <link>https://therealmjp.github.io/posts/d3d11-features/</link>
      <pubDate>Mon, 15 Nov 2010 02:24:48 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/d3d11-features/</guid>
      <description>D3D11 came with a whole bunch of new big-ticket features that received plenty of attention and publicity. Things like tessellation, compute shaders, and multithreaded command submission have the subject of many presentations, discussion, and sample apps. However D3D11 also came with a few other features that allow more &amp;ldquo;traditional&amp;rdquo; rendering approaches to benefit from the increased programmability of graphics hardware. Unfortunately most of them have gone relatively unnoticed, which isn&amp;rsquo;t surprising when you consider that most of them have little or no documentation, (much like some of the cool stuff that came in D3D10.</description>
    </item>
    
    <item>
      <title>Position From Depth 3: Back In The Habit</title>
      <link>https://therealmjp.github.io/posts/position-from-depth-3/</link>
      <pubDate>Mon, 06 Sep 2010 07:11:52 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/position-from-depth-3/</guid>
      <description>A friend of mine once told me that you could use &amp;ldquo;back in the habit&amp;rdquo; as the subtitle for any movie sequel. I think it works.
So a lot of people still have trouble with reconstructing position from depth thing, judging by the emails I get and also the threads I see in the gamedev forums made by people who read my earlier blog posts. Can&amp;rsquo;t say I blame them&amp;hellip;it&amp;rsquo;s pretty tricky, and easy to screw up.</description>
    </item>
    
    <item>
      <title>Deferred MSAA</title>
      <link>https://therealmjp.github.io/posts/deferred-msaa/</link>
      <pubDate>Mon, 16 Aug 2010 08:57:37 +0000</pubDate>
      
      <guid>https://therealmjp.github.io/posts/deferred-msaa/</guid>
      <description>A long while ago I was looking into morphological antialiasing (MLAA) to see if I could somehow make it practical for a GPU that isn&amp;rsquo;t the latest monster from Nvidia or ATI. With MLAA most people talk about how nicely it cleans up edges (which it certainly does), but for me the really cool part is how it&amp;rsquo;s completely orthogonal to the technique used to render the image. It could have been rasterized and forward rendered, it could be the product of a deferred rendering, or it could even be ray-traced: in all cases the algorithm works the same.</description>
    </item>
    
  </channel>
</rss>